{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (Advanced Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Basically what we do here is examine a dataset with E-commerce Customer Data for a company's website and mobile application. Then we want to see if we can build a regression model that will predict the customer's yearly spend on the company's product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/02 01:55:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/10/02 01:55:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('linear_regression_adv').getOrCreate()\n",
    "\n",
    "# If you're getting an error with numpy, please type 'sudo pip install numpy --user' into the EC2 console.\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Spark to read in the Ecommerce Customers csv file. You can infer csv schemas. \n",
    "data = spark.read.csv(\"Datasets/ecommerce_data.csv\",inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Avatar: string (nullable = true)\n",
      " |-- Avg Session Length: double (nullable = true)\n",
      " |-- Time on App: double (nullable = true)\n",
      " |-- Time on Website: double (nullable = true)\n",
      " |-- Length of Membership: double (nullable = true)\n",
      " |-- Yearly Amount Spent: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the DataFrame. You can see potential features as well as the predictor.\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Email='mstephenson@fernandez.com', Address='835 Frank TunnelWrightmouth, MI 82180-9605', Avatar='Violet', Avg Session Length=34.49726772511229, Time on App=12.65565114916675, Time on Website=39.57766801952616, Length of Membership=4.0826206329529615, Yearly Amount Spent=587.9510539684005)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's focus on one row to make it easier to read.\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mstephenson@fernandez.com\n",
      "835 Frank TunnelWrightmouth, MI 82180-9605\n",
      "Violet\n",
      "34.49726772511229\n",
      "12.65565114916675\n",
      "39.57766801952616\n",
      "4.0826206329529615\n",
      "587.9510539684005\n"
     ]
    }
   ],
   "source": [
    "# A simple for loop allows us to make it even clearer. \n",
    "for item in data.head():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up a DataFrame for Machine Learning (MLlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do a few things before Spark can accept the data for machine learning. First of all, it needs to be in the form of two columns: label and features. Unlike the documentation example, this data is messy. We'll need to combine all of the features into a single vector. VectorAssembler simplifies the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import VectorAssembler and Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input columns are the feature column names, and the output column is what you'd like the new column to be named. \n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Avg Session Length\", \"Time on App\", \n",
    "               \"Time on Website\",'Length of Membership'],\n",
    "    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've created the assembler variable, let's actually transform the data.\n",
    "output = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Avatar: string (nullable = true)\n",
      " |-- Avg Session Length: double (nullable = true)\n",
      " |-- Time on App: double (nullable = true)\n",
      " |-- Time on Website: double (nullable = true)\n",
      " |-- Length of Membership: double (nullable = true)\n",
      " |-- Yearly Amount Spent: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Email='mstephenson@fernandez.com', Address='835 Frank TunnelWrightmouth, MI 82180-9605', Avatar='Violet', Avg Session Length=34.49726772511229, Time on App=12.65565114916675, Time on Website=39.57766801952616, Length of Membership=4.0826206329529615, Yearly Amount Spent=587.9510539684005, features=DenseVector([34.4973, 12.6557, 39.5777, 4.0826]))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using print schema, you see that the features output column has been added. \n",
    "output.printSchema()\n",
    "\n",
    "# You can see that the features column is a dense vector that combines the various features as expected.\n",
    "output.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|            features|Yearly Amount Spent|\n",
      "+--------------------+-------------------+\n",
      "|[34.4972677251122...|  587.9510539684005|\n",
      "|[31.9262720263601...|  392.2049334443264|\n",
      "|[33.0009147556426...| 487.54750486747207|\n",
      "|[34.3055566297555...|  581.8523440352177|\n",
      "|[33.3306725236463...|  599.4060920457634|\n",
      "|[33.8710378793419...|   637.102447915074|\n",
      "|[32.0215955013870...|  521.5721747578274|\n",
      "|[32.7391429383803...|  549.9041461052942|\n",
      "|[33.9877728956856...|  570.2004089636196|\n",
      "|[31.9365486184489...|  427.1993848953282|\n",
      "|[33.9925727749537...|  492.6060127179966|\n",
      "|[33.8793608248049...|  522.3374046069357|\n",
      "|[29.5324289670579...|  408.6403510726275|\n",
      "|[33.1903340437226...|  573.4158673313865|\n",
      "|[32.3879758531538...|  470.4527333009554|\n",
      "|[30.7377203726281...|  461.7807421962299|\n",
      "|[32.1253868972878...| 457.84769594494855|\n",
      "|[32.3388993230671...| 407.70454754954415|\n",
      "|[32.1878120459321...|  452.3156754800354|\n",
      "|[32.6178560628234...|   605.061038804892|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's select two columns (the feature and predictor).\n",
    "# This is now in the appropriate format to be processed by Spark.\n",
    "final_data = output.select(\"features\",'Yearly Amount Spent')\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a randomised 70/30 split. \n",
    "# Remember, you can use other splits depending on how easy/difficult it is to train your model.\n",
    "train_data,test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|Yearly Amount Spent|\n",
      "+-------+-------------------+\n",
      "|  count|                360|\n",
      "|   mean|  498.0971226357352|\n",
      "| stddev|  79.73375726559242|\n",
      "|    min| 256.67058229005585|\n",
      "|    max|  725.5848140556806|\n",
      "+-------+-------------------+\n",
      "\n",
      "+-------+-------------------+\n",
      "|summary|Yearly Amount Spent|\n",
      "+-------+-------------------+\n",
      "|  count|                140|\n",
      "|   mean| 502.44324986021957|\n",
      "| stddev|  78.42377012673346|\n",
      "|    min| 302.18954780965197|\n",
      "|    max|  765.5184619388373|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see our training data.\n",
    "train_data.describe().show()\n",
    "\n",
    "# And our testing data.\n",
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a Linear Regression Model object. Because the feature column is named 'features', we don't have to worry about it. However, as the labelCol isn't the default name, we have to specify it's name (Yearly Amount Spent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(labelCol='Yearly Amount Spent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/02 01:57:30 WARN Instrumentation: [9134012c] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/10/02 01:57:30 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/10/02 01:57:30 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/10/02 01:57:30 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    }
   ],
   "source": [
    "# Fit the model to the data.\n",
    "lrModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [25.71956724841068,39.043619650661086,0.45276721049979984,62.463702868266985] Intercept: -1059.3620709335287\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for linear regression.\n",
    "print(\"Coefficients: {} Intercept: {}\".format(lrModel.coefficients,lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's evaluate the model against the test data.\n",
    "test_results = lrModel.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "| -9.845237739732625|\n",
      "| -8.334245930444922|\n",
      "|  3.747652549159625|\n",
      "|  4.094557252741765|\n",
      "|  4.442266821418002|\n",
      "|  3.107264740466178|\n",
      "|-15.074228757230458|\n",
      "|-2.5796769839276976|\n",
      "|  18.59969332175001|\n",
      "|  7.207409651807382|\n",
      "|-26.472190429289526|\n",
      "| -7.839883789678765|\n",
      "|-1.6609697003750057|\n",
      "|    8.3829877691266|\n",
      "|  -9.35510773574481|\n",
      "|-18.566025324536668|\n",
      "| 0.5740960177401462|\n",
      "| -7.992582650199665|\n",
      "|  4.244427677161525|\n",
      "|  23.49701816499811|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RSME: 10.541561536710073\n"
     ]
    }
   ],
   "source": [
    "# Interesting results! This shows the difference between the predicted value and the test data.\n",
    "test_results.residuals.show()\n",
    "\n",
    "# Let's get some evaluation metrics (as discussed in the previous linear regression notebook).\n",
    "print(\"RSME: {}\".format(test_results.rootMeanSquaredError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.98180183079438\n"
     ]
    }
   ],
   "source": [
    "# We can also get the R2 value. \n",
    "print(\"R2: {}\".format(test_results.r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at RMSE and R2, we can see that the model is quite accurate. The RMSE shows that, on average, there's only a \\\\$10 discrepancy between the actual and predicted results. Comparing this to the table below, the average amount spent (\\\\$499) and standard deviation (\\\\$79), a \\\\$10 error is surprisingly good. \n",
    "\n",
    "The R2 also shows that the model accounts for 98% of the variance in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|Yearly Amount Spent|\n",
      "+-------+-------------------+\n",
      "|  count|                500|\n",
      "|   mean|  499.3140382585909|\n",
      "| stddev|   79.3147815497068|\n",
      "|    min| 256.67058229005585|\n",
      "|    max|  765.5184619388373|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But what if we didn't have the predictor data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't really relevant to your assignment, but useful in a real-world scenario. What if you have all of these features but no predictor data? How do you actually use the model you've created? Check out the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[30.3931845423455...|\n",
      "|[31.1280900496166...|\n",
      "|[31.3091926408918...|\n",
      "|[31.3584771924370...|\n",
      "|[31.3662121671876...|\n",
      "|[31.4459724827577...|\n",
      "|[31.5741380228732...|\n",
      "|[31.5761319713222...|\n",
      "|[31.6005122003032...|\n",
      "|[31.6548096756927...|\n",
      "|[31.6739155032749...|\n",
      "|[31.7207699002873...|\n",
      "|[31.8186165667690...|\n",
      "|[31.8209982016720...|\n",
      "|[31.8648325480987...|\n",
      "|[31.9563005605233...|\n",
      "|[32.0047530203648...|\n",
      "|[32.0085045178551...|\n",
      "|[32.0305497162129...|\n",
      "|[32.0498393904573...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's just select the features column (removing the label column).\n",
    "unlabeled_data = test_data.select('features')\n",
    "unlabeled_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can transform the unlabeled data.\n",
    "predictions = lrModel.transform(unlabeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|            features|        prediction|\n",
      "+--------------------+------------------+\n",
      "|[30.3931845423455...|329.77410754292623|\n",
      "|[31.1280900496166...| 565.5869326774996|\n",
      "|[31.3091926408918...|  428.973065290774|\n",
      "|[31.3584771924370...|491.08139319673364|\n",
      "|[31.3662121671876...| 426.1466157350669|\n",
      "|[31.4459724827577...| 481.7697001946624|\n",
      "|[31.5741380228732...| 559.4835009178173|\n",
      "|[31.5761319713222...|  543.806260973256|\n",
      "|[31.6005122003032...| 460.5731581693469|\n",
      "|[31.6548096756927...|468.05601407574113|\n",
      "|[31.6739155032749...|502.19725833917073|\n",
      "|[31.7207699002873...| 546.6148172677017|\n",
      "|[31.8186165667690...|448.07964307051066|\n",
      "|[31.8209982016720...|416.29229324408675|\n",
      "|[31.8648325480987...| 449.2463882125585|\n",
      "|[31.9563005605233...| 565.6919570717355|\n",
      "|[32.0047530203648...| 463.1718851028893|\n",
      "|[32.0085045178551...|451.18980367895506|\n",
      "|[32.0305497162129...| 590.0300557414503|\n",
      "|[32.0498393904573...| 455.2223387092172|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([30.3932, 11.803, 36.3158, 2.0838]), prediction=329.77410754292623)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It worked! Feeding the unlabeled data features into the model results in a prediction, \n",
    "# which is the amount someone with those features is likely to spend in a year.\n",
    "predictions.show()\n",
    "predictions.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
